num_cestas = floor(runif(25, min = 0, max = 50))
)
View(tabela_basquete)
summary(tabela_basquete)
# Utilizando a função tapply()
tapply(tabela_basquete$num_cestas, tabela_basquete$Equipe, sum)
tapply(tabela_basquete$num_cestas, tabela_basquete$Equipe, mean)
# Exercicio 3 - Considere o dataframe abaixo.
# Calcule a média por disciplina e depois calcule a média de apenas uma disciplina
escola <- data.frame(Aluno = c('Alan', 'Alice', 'Alana', 'Aline', 'Alex', 'Ajay'),
Matematica = c(90, 80, 85, 87, 56, 79),
Geografia = c(100, 78, 86, 90, 98, 67),
Quimica = c(76, 56, 89, 90, 100, 87))
escola
tapply(escola[2:4], escola[2:4], mean)
?tapply?
tapply(escola[2:4], escola[2:4], mean)
?tapply?
tapply(escola[2:4], escola[2:4], mean)
?tapply?
tapply(escola[2:4], escola[2:4], mean)
?tapply
as.matrix(escola[2:4])
apply(as.matrix(escola[2:4]), 2, mean)
apply(as.vector(escola[2]), 2, mean)
# Exercicio 3 - Considere o dataframe abaixo.
# Calcule a média por disciplina e depois calcule a média de apenas uma disciplina
escola <- data.frame(Aluno = c('Alan', 'Alice', 'Alana', 'Aline', 'Alex', 'Ajay'),
Matematica = c(90, 80, 85, 87, 56, 79),
Geografia = c(100, 78, 86, 90, 98, 67),
Quimica = c(76, 56, 89, 90, 100, 87))
escola
apply(as.matrix(escola[2:4]), 2, mean)
apply(as.vector(escola[2]), 2, mean)
lst <- list(c(2, 3, 4))
lst
lapply(lst, sum)
# Exercicio 5 - Transforme a lista anterior um vetor
vetor <- as.vector(lst)
vetor
class(vetor)
# Exercicio 5 - Transforme a lista anterior um vetor
vetor <- as.array(lst)
vetor
class(vetor)
# Exercicio 6 - Considere a string abaixo. Substitua a palavra "textos" por "frases"
str <- c("Expressoes", "regulares", "em linguagem R",
"permitem a busca de padroes", "e exploracao de textos",
"podemos buscar padroes em digitos",
"como por exemplo",
"10992451280")
str
gsub("textos", "frases", str)
# Exercicio 7 - Usando o dataset mtcars, crie um gráfico com ggplot do tipo
# scatter plot. Use as colunas disp e mpg nos eixos x e y respectivamente
library(ggplot2)
ggplot() +
ls(ggplot2)
ggplot() +
ls("ggplot2")
ggplot() +
lsf.str("package:ggplot2")
ggplot() + geom_point(aes(x = mtcars$disp, y = mtcars$mpg))
# Exercicio 8 - Considere a matriz abaixo.
# Crie um bar plot que represente os dados em barras individuais.
mat1 <- matrix(c(652,1537,598,242,36,46,38,21,218,327,106,67), nrow = 3, byrow = T)
mat1
?mtcars
ggplot() + geom_point(aes(x = mtcars$disp, y = mtcars$mpg), color = "red", xlab = "Displacement", ylab = "Miles per galon")
?geom_point
?aes
ggplot() + geom_point(aes(x = mtcars$disp, y = mtcars$mpg), color = "red") +
xlab("Displacement") + ylab("Miles per galon")
dimnames(mat1) <- list(c("A", "B", "C"), c("a", "b", "c", "d"))
mat1
ggplot() + geom_bar(data = mat1)
ggplot() + geom_bar(data = as.data.frame(mat1))
ggplot() + geom_bar(aes(mat1))
ggplot(mat1) + geom_bar()
ggplot(data = as.data.frame(mat1)) + geom_bar()
data(diamonds)
ggplot(data = diamonds, aes(x = price, group = fill, fill = cut)) +
geom_density(adjust = 1.5)
ggplot(data = diamonds, aes(x = price, group = fill)) +
geom_density(adjust = 1.5)
View(diamonds)
##################################################################
#### Projeto 1 - Detecção de fraudes - TalkingData AdTracking ####
##################################################################
setwd("C:/DataScience/FCD/BigDataAnalytics-R-Azure/Projeto-1/")
getwd()
############################################################
#### Carregamento de dados e bibliotecas ###################
############################################################
library(dplyr)
library(ggplot2)
library(randomForest)
library(caret)
library(data.table)
library(e1071)
# Utilizando para testes e desenvolvimento na máquina local o arquivo train_sample.csv
# por motivos de menor utilização de memória da máquina. Na versão para produção, será utilizado
# o arquivo completo train.csv
dados<- read.csv("train_sample.csv")
head(dados)
##############################################
#### Análise Exploratória dos Dados ##########
##############################################
# Limpeza e manipulação
# Verificando a existência de NAs no arquivo
variaveis <- names(dados)
for(v in variaveis){
print('-------------')
print(paste("Número de NAs na variável", v))
print(sum(is.na(dados[v])))
}
str(dados)
dados_filt <- dados %>% filter(attributed_time != '' & is_attributed != 1)
head(dados_filt)
dados %>%
group_by(is_attributed) %>%
summarise(Vazio = sum(attributed_time == ''),
Alguma_coisa = sum(attributed_time != ''))
dados$attributed_time <- NULL
rm(dados_filt)
# Tratando a variável click_time: pode-se avaliar o tempo tanto como uma variável contínua,
# no sentido de se analisar uma tendência de aumento do número de fraudes ao longo do tempo
# como também de forma categorizada (por mês, dia do mês, dia da semana, hora do dia, etc.)
# para analisar possíveis sazonalidades no número de fraudes ao longo do ano.
# Para isto será criada uma variável click_time_posixct, que é a conversão da variável
# click_time para o formato POSIXct. Poderão ser criadas variáveis como rel_click_time,
# que será o tempo relativo entre o menor tempo do dataset e cada tempo registrado,
# e também variáveis categóricas como year, month, week, weekday, hour, minute, second.
# A variável original click_time será eliminada do dataset.
dados$click_time_posixct <- as.POSIXct(dados$click_time)
dados$click_time <- NULL
t_1st <- min(dados$click_time_posixct)
t_last <- max(dados$click_time_posixct)
t_last - t_1st
# Como o tempo decorrido entre o primeiro e o último clique é de apenas 3 dias, não parece
# fazer sentido um aumento do número de fraudes significativo ao longo desse curto intervalo de
# tempo. De modo que as variáveis  year, month, week, weekday não serão criadas para poupar
# esforço computacional. Serão criadas, então, apenas as variáveis hour, minute, second.
# Também para reduzir esforço computacional, será eliminada do dataset de treino a variável
# click_time_posixct.
dados$hour <- hour(dados$click_time_posixct)
dados$minute <- minute(dados$click_time_posixct)
dados$second <- second(dados$click_time_posixct)
dados$click_time_posixct <- NULL
str(dados)
summary(dados)
# Inspeção gráfica de relações entre os atributos e o label.
ggplot(dados, aes(is_attributed)) + geom_bar(fill = "blue", alpha = 0.5) +
ggtitle("Totais de is_attributed por categoria")
table(dados$is_attributed)
variaveis <- names(dados)
variaveis <- variaveis[variaveis != "is_attributed"]
lapply(variaveis, function(x){
ggplot(dados, aes_string(x)) +
geom_histogram(fill = "blue", alpha = 0.5) +
ggtitle(paste("Histograma de",x)) +
facet_grid(dados$is_attributed ~ .)})
# Balanceando o dataset
# Criando um arquivo CSV para carregar no Azure ML
write.csv(dados, "dados_para_balancear.csv", row.names = FALSE)
# Lendo um arquivo CSV retornado pelo Azure ML
dados <- read.csv("dados_balanceados.csv")
str(dados)
# Retornando o label para o tipo fator
dados$is_attributed <- factor(dados$is_attributed)
# Inspeção gráfica após o balanceamento
ggplot(dados, aes(is_attributed)) + geom_bar(fill = "blue", alpha = 0.5) +
ggtitle("Totais de is_attributed por categoria após balanceamento")
table(dados$is_attributed)
variaveis <- names(dados)
variaveis <- variaveis[variaveis != "is_attributed"]
lapply(variaveis, function(x){
ggplot(dados, aes_string(x)) +
geom_histogram(fill = "blue", alpha = 0.5) +
ggtitle(paste("Histograma de",x,"após balanceamento")) +
facet_grid(dados$is_attributed ~ .)})
## Criando um dataset categorizado para posterior teste dos modelos
variaveis <- names(dados)
dados_cat <- dados
for(v in variaveis){
dados_cat[[v]] <- factor(dados_cat[[v]])
}
str(dados_cat)
dados_cat <- dados
variaveis <- names(dados_cat)
variaveis <- variaveis[variaveis != "hour"]
for(v in variaveis){
inc = (max(dados_cat[[v]])-min(dados_cat[[v]]))/52 # para dividir em 53 níveis
xmin = min(dados_cat[[v]])
dados_cat[[v]] <- (dados_cat[[v]] - xmin)/inc
dados_cat[[v]] <- factor(floor(dados_cat[[v]]))
}
##################################################################
#### Projeto 1 - Detecção de fraudes - TalkingData AdTracking ####
##################################################################
setwd("C:/DataScience/FCD/BigDataAnalytics-R-Azure/Projeto-1/")
getwd()
############################################################
#### Carregamento de dados e bibliotecas ###################
############################################################
library(dplyr)
library(ggplot2)
library(randomForest)
library(caret)
library(data.table)
library(e1071)
# Utilizando para testes e desenvolvimento na máquina local o arquivo train_sample.csv
# por motivos de menor utilização de memória da máquina. Na versão para produção, será utilizado
# o arquivo completo train.csv
dados<- read.csv("train_sample.csv")
head(dados)
##############################################
#### Análise Exploratória dos Dados ##########
##############################################
# Limpeza e manipulação
# Verificando a existência de NAs no arquivo
variaveis <- names(dados)
for(v in variaveis){
print('-------------')
print(paste("Número de NAs na variável", v))
print(sum(is.na(dados[v])))
}
str(dados)
dados_filt <- dados %>% filter(attributed_time != '' & is_attributed != 1)
head(dados_filt)
dados %>%
group_by(is_attributed) %>%
summarise(Vazio = sum(attributed_time == ''),
Alguma_coisa = sum(attributed_time != ''))
dados$attributed_time <- NULL
rm(dados_filt)
# Tratando a variável click_time: pode-se avaliar o tempo tanto como uma variável contínua,
# no sentido de se analisar uma tendência de aumento do número de fraudes ao longo do tempo
# como também de forma categorizada (por mês, dia do mês, dia da semana, hora do dia, etc.)
# para analisar possíveis sazonalidades no número de fraudes ao longo do ano.
# Para isto será criada uma variável click_time_posixct, que é a conversão da variável
# click_time para o formato POSIXct. Poderão ser criadas variáveis como rel_click_time,
# que será o tempo relativo entre o menor tempo do dataset e cada tempo registrado,
# e também variáveis categóricas como year, month, week, weekday, hour, minute, second.
# A variável original click_time será eliminada do dataset.
dados$click_time_posixct <- as.POSIXct(dados$click_time)
dados$click_time <- NULL
t_1st <- min(dados$click_time_posixct)
t_last <- max(dados$click_time_posixct)
t_last - t_1st
# Como o tempo decorrido entre o primeiro e o último clique é de apenas 3 dias, não parece
# fazer sentido um aumento do número de fraudes significativo ao longo desse curto intervalo de
# tempo. De modo que as variáveis  year, month, week, weekday não serão criadas para poupar
# esforço computacional. Serão criadas, então, apenas as variáveis hour, minute, second.
# Também para reduzir esforço computacional, será eliminada do dataset de treino a variável
# click_time_posixct.
dados$hour <- hour(dados$click_time_posixct)
dados$minute <- minute(dados$click_time_posixct)
dados$second <- second(dados$click_time_posixct)
dados$click_time_posixct <- NULL
str(dados)
summary(dados)
# Inspeção gráfica de relações entre os atributos e o label.
ggplot(dados, aes(is_attributed)) + geom_bar(fill = "blue", alpha = 0.5) +
ggtitle("Totais de is_attributed por categoria")
table(dados$is_attributed)
variaveis <- names(dados)
variaveis <- variaveis[variaveis != "is_attributed"]
lapply(variaveis, function(x){
ggplot(dados, aes_string(x)) +
geom_histogram(fill = "blue", alpha = 0.5) +
ggtitle(paste("Histograma de",x)) +
facet_grid(dados$is_attributed ~ .)})
# Balanceando o dataset
# Criando um arquivo CSV para carregar no Azure ML
write.csv(dados, "dados_para_balancear.csv", row.names = FALSE)
# Lendo um arquivo CSV retornado pelo Azure ML
dados <- read.csv("dados_balanceados.csv")
str(dados)
# Retornando o label para o tipo fator
dados$is_attributed <- factor(dados$is_attributed)
# Inspeção gráfica após o balanceamento
ggplot(dados, aes(is_attributed)) + geom_bar(fill = "blue", alpha = 0.5) +
ggtitle("Totais de is_attributed por categoria após balanceamento")
table(dados$is_attributed)
variaveis <- names(dados)
variaveis <- variaveis[variaveis != "is_attributed"]
lapply(variaveis, function(x){
ggplot(dados, aes_string(x)) +
geom_histogram(fill = "blue", alpha = 0.5) +
ggtitle(paste("Histograma de",x,"após balanceamento")) +
facet_grid(dados$is_attributed ~ .)})
## Criando um dataset categorizado para posterior teste dos modelos
variaveis <- names(dados)
dados_cat <- dados
for(v in variaveis){
dados_cat[[v]] <- factor(dados_cat[[v]])
}
str(dados_cat)
dados_cat <- dados
variaveis <- names(dados_cat)
variaveis <- variaveis[variaveis != "hour"]
for(v in variaveis){
inc = (max(dados_cat[[v]])-min(dados_cat[[v]]))/52 # para dividir em 53 níveis
xmin = min(dados_cat[[v]])
dados_cat[[v]] <- (dados_cat[[v]] - xmin)/inc
dados_cat[[v]] <- factor(floor(dados_cat[[v]]))
}
# Categorizando também o atributo hour, que tem menos que 53 níveis
dados_cat$hour <- factor(dados_cat$hour)
str(dados_cat)
str(dados)
# Fazendo o split data entre dados de treino e dados de validação, uma vez que os dados
# de teste não tem label disponível.
split <- createDataPartition(y = dados$is_attributed, p = 0.7, list = FALSE)
dados_treino <- dados[split, ]
dados_valid <- dados[-split, ]
dados_cat_treino <- dados_cat[split, ]
dados_cat_valid <- dados_cat[-split, ]
for(v in variaveis){
inc = (max(dados_cat[[v]])-min(dados_cat[[v]]))/52 # para dividir em 53 níveis
xmin = min(dados_cat[[v]])
dados_cat[[v]] <- (dados_cat[[v]] - xmin)/inc
dados_cat[[v]] <- factor(floor(dados_cat[[v]]))
}
##################################################################
#### Projeto 1 - Detecção de fraudes - TalkingData AdTracking ####
##################################################################
setwd("C:/DataScience/FCD/BigDataAnalytics-R-Azure/Projeto-1/")
getwd()
############################################################
#### Carregamento de dados e bibliotecas ###################
############################################################
library(dplyr)
library(ggplot2)
library(randomForest)
library(caret)
library(data.table)
library(e1071)
# Utilizando para testes e desenvolvimento na máquina local o arquivo train_sample.csv
# por motivos de menor utilização de memória da máquina. Na versão para produção, será utilizado
# o arquivo completo train.csv
dados<- read.csv("train_sample.csv")
head(dados)
##############################################
#### Análise Exploratória dos Dados ##########
##############################################
# Limpeza e manipulação
# Verificando a existência de NAs no arquivo
variaveis <- names(dados)
for(v in variaveis){
print('-------------')
print(paste("Número de NAs na variável", v))
print(sum(is.na(dados[v])))
}
str(dados)
dados_filt <- dados %>% filter(attributed_time != '' & is_attributed != 1)
head(dados_filt)
dados %>%
group_by(is_attributed) %>%
summarise(Vazio = sum(attributed_time == ''),
Alguma_coisa = sum(attributed_time != ''))
dados$attributed_time <- NULL
rm(dados_filt)
# Tratando a variável click_time: pode-se avaliar o tempo tanto como uma variável contínua,
# no sentido de se analisar uma tendência de aumento do número de fraudes ao longo do tempo
# como também de forma categorizada (por mês, dia do mês, dia da semana, hora do dia, etc.)
# para analisar possíveis sazonalidades no número de fraudes ao longo do ano.
# Para isto será criada uma variável click_time_posixct, que é a conversão da variável
# click_time para o formato POSIXct. Poderão ser criadas variáveis como rel_click_time,
# que será o tempo relativo entre o menor tempo do dataset e cada tempo registrado,
# e também variáveis categóricas como year, month, week, weekday, hour, minute, second.
# A variável original click_time será eliminada do dataset.
dados$click_time_posixct <- as.POSIXct(dados$click_time)
dados$click_time <- NULL
t_1st <- min(dados$click_time_posixct)
t_last <- max(dados$click_time_posixct)
t_last - t_1st
# Como o tempo decorrido entre o primeiro e o último clique é de apenas 3 dias, não parece
# fazer sentido um aumento do número de fraudes significativo ao longo desse curto intervalo de
# tempo. De modo que as variáveis  year, month, week, weekday não serão criadas para poupar
# esforço computacional. Serão criadas, então, apenas as variáveis hour, minute, second.
# Também para reduzir esforço computacional, será eliminada do dataset de treino a variável
# click_time_posixct.
dados$hour <- hour(dados$click_time_posixct)
dados$minute <- minute(dados$click_time_posixct)
dados$second <- second(dados$click_time_posixct)
dados$click_time_posixct <- NULL
str(dados)
summary(dados)
# Inspeção gráfica de relações entre os atributos e o label.
ggplot(dados, aes(is_attributed)) + geom_bar(fill = "blue", alpha = 0.5) +
ggtitle("Totais de is_attributed por categoria")
table(dados$is_attributed)
variaveis <- names(dados)
variaveis <- variaveis[variaveis != "is_attributed"]
lapply(variaveis, function(x){
ggplot(dados, aes_string(x)) +
geom_histogram(fill = "blue", alpha = 0.5) +
ggtitle(paste("Histograma de",x)) +
facet_grid(dados$is_attributed ~ .)})
# Balanceando o dataset
# Criando um arquivo CSV para carregar no Azure ML
write.csv(dados, "dados_para_balancear.csv", row.names = FALSE)
# Lendo um arquivo CSV retornado pelo Azure ML
dados <- read.csv("dados_balanceados.csv")
str(dados)
# Retornando o label para o tipo fator
dados$is_attributed <- factor(dados$is_attributed)
# Inspeção gráfica após o balanceamento
ggplot(dados, aes(is_attributed)) + geom_bar(fill = "blue", alpha = 0.5) +
ggtitle("Totais de is_attributed por categoria após balanceamento")
table(dados$is_attributed)
variaveis <- names(dados)
variaveis <- variaveis[variaveis != "is_attributed"]
lapply(variaveis, function(x){
ggplot(dados, aes_string(x)) +
geom_histogram(fill = "blue", alpha = 0.5) +
ggtitle(paste("Histograma de",x,"após balanceamento")) +
facet_grid(dados$is_attributed ~ .)})
## Criando um dataset categorizado para posterior teste dos modelos
variaveis <- names(dados)
dados_cat <- dados
for(v in variaveis){
dados_cat[[v]] <- factor(dados_cat[[v]])
}
str(dados_cat)
dados_cat <- dados
variaveis <- names(dados_cat)
variaveis <- variaveis[variaveis != "hour"]
for(v in variaveis){
inc = (max(dados_cat[[v]])-min(dados_cat[[v]]))/52 # para dividir em 53 níveis
xmin = min(dados_cat[[v]])
dados_cat[[v]] <- (dados_cat[[v]] - xmin)/inc
dados_cat[[v]] <- factor(floor(dados_cat[[v]]))
}
# Categorizando também o atributo hour, que tem menos que 53 níveis
dados_cat$hour <- factor(dados_cat$hour)
str(dados_cat)
str(dados)
# Fazendo o split data entre dados de treino e dados de validação, uma vez que os dados
# de teste não tem label disponível.
split <- createDataPartition(y = dados$is_attributed, p = 0.7, list = FALSE)
dados_treino <- dados[split, ]
dados_valid <- dados[-split, ]
dados_cat_treino <- dados_cat[split, ]
dados_cat_valid <- dados_cat[-split, ]
## Feature selection
# Função para seleção de variáveis
#run.feature.selection <- function(num.iters=20, feature.vars, class.var){
#  set.seed(10)
#  variable.sizes <- 1:10
#  control <- rfeControl(functions = rfFuncs, method = "cv",
#                        verbose = FALSE, returnResamp = "all",
#                        number = num.iters)
#  results.rfe <- rfe(x = feature.vars, y = class.var,
#                     sizes = variable.sizes,
#                     rfeControl = control)
#  return(results.rfe)
#}
#head(dados)
# Executando a função para o dataset não-categorizado
#rfe.results <- run.feature.selection(feature.vars = dados[,-6],
#                                     class.var = dados[,6])
# Visualizando os resultados
#rfe.results
#varImp((rfe.results))
## Feature selection
#formula <- "is_attributed ~ ."
#formula <- as.formula(formula)
#control <- trainControl(method = "repeatedcv", number = 10, repeats = 2)
#model <- train(formula, data = data_train, method = "glm", trControl = control)
#importance <- varImp(model, scale = FALSE)
#plot(importance)
###################################################
#### Primeiro modelo de Machine Learning ##########
###################################################
# Algoritmo: randomForest
# Atributos: não-categorizados
modelo_rf1 <- randomForest(is_attributed ~ .,
data = dados_treino,
ntree = 100, nodesize = 10, importance = T)
previsao_rf1 <- predict(modelo_rf1, dados_valid, type = 'class')
# Matriz de confusão
confusionMatrix(previsao_rf1, dados_valid$is_attributed)
###################################################
#### Segundo modelo de Machine Learning ###########
###################################################
# Algoritmo: randomForest
# Atributos: categorizados
modelo_rf2 <- randomForest(is_attributed ~ .,
data = dados_cat_treino,
ntree = 100, nodesize = 10, importance = T)
previsao_rf2 <- predict(modelo_rf2, dados_cat_valid, type = 'class')
# Matriz de confusão
confusionMatrix(previsao_rf2, dados_cat_valid$is_attributed)
###################################################
#### Terceiro modelo de Machine Learning ##########
###################################################
# Algoritmo: SVM
# Atributos: não-categorizados
modelo_svm1 <- svm(is_attributed ~ .,
data = dados_treino,
type = 'C-classification',
kernel = 'radial')
previsao_svm1 <- predict(modelo_svm1, dados_valid, type = 'class')
# Matriz de confusão
confusionMatrix(previsao_svm1, dados_valid$is_attributed)
###################################################
#### Quarto modelo de Machine Learning ############
###################################################
# Algoritmo: SVM
