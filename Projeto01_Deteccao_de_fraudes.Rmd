---
title: "Projeto_1_Deteccao_fraudes"
author: "Rafael Schena"
date: "20/11/2020"
output:
  html_document:
    df_print: paged
---
# Fromação Cientista de Dados - Projeto 1
## Big Data Analytics com R e Microsoft Azure Machine Learning
## Detecção de Fraudes no Tráfego de Cliques em Propagandas de Aplicações Mobile

### Definição do problema de negócio
A TalkingData (https://www.talkingdata.com), a maior plataforma de Big
Data independente da China, cobre mais de 70% dos dispositivos móveis ativos
em todo o país. Eles lidam com 3 bilhões de cliques por dia, dos quais 90% são
potencialmente fraudulentos.

O objetivo deste trabalho é construir um modelo de machine learning que possa determinar se um clique é fraudulento ou não com base em dados históricos fornecidos pela empresa
e disponíveis publicamente em (https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection/data)

### Análise exploratória dos dados

Descrição dos dados fornecidos:

File descriptions
train.csv - the training set
train_sample.csv - 100,000 randomly-selected rows of training data, to inspect data before downloading full set
test.csv - the test set
sampleSubmission.csv - a sample submission file in the correct format
UPDATE: test_supplement.csv - This is a larger test set that was unintentionally released at the start of the competition. It is not necessary to use this data, but it is permitted to do so. The official test data is a subset of this data.
Data fields
Each row of the training data contains a click record, with the following features.

ip: ip address of click.
app: app id for marketing.
device: device type id of user mobile phone (e.g., iphone 6 plus, iphone 7, huawei mate 7, etc.)
os: os version id of user mobile phone
channel: channel id of mobile ad publisher
click_time: timestamp of click (UTC)
attributed_time: if user download the app for after clicking an ad, this is the time of the app download
is_attributed: the target that is to be predicted, indicating the app was downloaded
Note that ip, app, device, os, and channel are encoded.

The test data is similar, with the following differences:

click_id: reference for making predictions
is_attributed: not included



```{r}
##################################################################
#### Projeto 1 - Detecção de fraudes - TalkingData AdTracking ####
##################################################################

setwd("C:/DataScience/FCD/BigDataAnalytics-R-Azure/Projeto-1/")
getwd()

############################################################
#### Carregamento de dados e bibliotecas ###################
############################################################

library(dplyr)
library(ggplot2)
library(randomForest)
library(caret)
library(data.table)
library(e1071)

# Utilizando para testes e desenvolvimento na máquina local o arquivo train_sample.csv
# por motivos de menor utilização de memória da máquina. Na versão para produção, será utilizado
# o arquivo completo train.csv

dados<- read.csv("train_sample.csv")

head(dados)


##############################################
#### Análise Exploratória dos Dados ##########
##############################################

# Limpeza e manipulação

# Verificando a existência de NAs no arquivo
variaveis <- names(dados)


for(v in variaveis){
  print('-------------')
  print(paste("Número de NAs na variável", v))
  print(sum(is.na(dados[v])))
}

str(dados)

dados_filt <- dados %>% filter(attributed_time != '' & is_attributed != 1)
head(dados_filt)

dados %>%
  group_by(is_attributed) %>%
  summarise(Vazio = sum(attributed_time == ''),
            Alguma_coisa = sum(attributed_time != ''))


dados$attributed_time <- NULL
rm(dados_filt)

# Tratando a variável click_time: pode-se avaliar o tempo tanto como uma variável contínua,
# no sentido de se analisar uma tendência de aumento do número de fraudes ao longo do tempo
# como também de forma categorizada (por mês, dia do mês, dia da semana, hora do dia, etc.)
# para analisar possíveis sazonalidades no número de fraudes ao longo do ano.

# Para isto será criada uma variável click_time_posixct, que é a conversão da variável
# click_time para o formato POSIXct. Poderão ser criadas variáveis como rel_click_time,
# que será o tempo relativo entre o menor tempo do dataset e cada tempo registrado,
# e também variáveis categóricas como year, month, week, weekday, hour, minute, second. 
# A variável original click_time será eliminada do dataset.

dados$click_time_posixct <- as.POSIXct(dados$click_time)
dados$click_time <- NULL

t_1st <- min(dados$click_time_posixct)
t_last <- max(dados$click_time_posixct)

t_last - t_1st

# Como o tempo decorrido entre o primeiro e o último clique é de apenas 3 dias, não parece
# fazer sentido um aumento do número de fraudes significativo ao longo desse curto intervalo de
# tempo. De modo que as variáveis  year, month, week, weekday não serão criadas para poupar
# esforço computacional. Serão criadas, então, apenas as variáveis hour, minute, second.
# Também para reduzir esforço computacional, será eliminada do dataset de treino a variável
# click_time_posixct.

dados$hour <- hour(dados$click_time_posixct)
dados$minute <- minute(dados$click_time_posixct)
dados$second <- second(dados$click_time_posixct)
dados$click_time_posixct <- NULL
str(dados)
summary(dados)


```
Não existem dados missing no conjunto de dados.

O dataset apresenta 7 atributos, sendo 2 do tipo fator e 5 do tipo inteiro.

A princípio, não faz sentido haver atributos do tipo inteiro, uma vez que eles não descrevem quantidades, mas sim categorias. Em testes preliminares, o algoritmo randomForest apresentou uma limitação em processar atributos com mais de 53 categorias. Serão construídos modelos de machine learning tanto com os dados fatorizados quanto com os dados nos formatos originais e serão avaliados os desempenhos.

A variável attributed_time, uma das variáveis tipo fator, não traz informação alguma, uma vez que ela só é preenchida quando o target é igual a 1, e nem está presente nos dados de teste. Assim sendo, foi excluída do dataset.


O outro atributo tipo fator (click_time) pode ser avaliada tanto como uma variável contínua no sentido de se analisar uma tendência de aumento do número de fraudes ao longo do tempo como também de forma categorizada (por mês, dia do mês, dia da semana, hora do dia, etc.) para analisar possíveis sazonalidades no número de fraudes ao longo do ano.

Para isto foi criada uma variável click_time_posixct, que é a conversão da variável click_time para o formato POSIXct. O intuito foi criar variáveis categóricas como year, month, week, weekday, hour, minute, second. A variável original click_time foi também eliminada do dataset.

Como o tempo decorrido entre o primeiro e o último clique é de apenas 3 dias, não parece fazer sentido um aumento do número de fraudes significativo ao longo desse curto intervalo de tempo. De modo que as variáveis  year, month, week, weekday não foram criadas. Foram criadas, então, apenas as variáveis hour, minute, second.
Após feitas estas manipulações, a variável click_time_posixct foi excluída para eliminar informações redundantes.

Próximo passo a ser realizado é uma análise exploratória dos dados.

```{r, echo = FALSE}
# Inspeção gráfica de relações entre os atributos e o label.

ggplot(dados, aes(is_attributed)) + geom_bar(fill = "blue", alpha = 0.5) +
  ggtitle("Totais de is_attributed por categoria")

table(dados$is_attributed)

variaveis <- names(dados)
variaveis <- variaveis[variaveis != "is_attributed"]


lapply(variaveis, function(x){
    ggplot(dados, aes_string(x)) +
      geom_histogram(fill = "blue", alpha = 0.5) +
      ggtitle(paste("Histograma de",x)) +
      facet_grid(dados$is_attributed ~ .)})


```
Constata-se que os dados desbalanceados de tal modo que torna impossível a avaliação de quaisquer relações entre as variáveis de entrada e o rótulo atribuído a cada observação.

Tal desbalanceamento também é prejudicial à construção de modelos de machine learning, de forma que deve ser adotada alguma técnica de balanceamento.

Para contornar este problema foi utilizada a técnica SMOTE (Synthetic Minority Oversampling Technique), que vai sintetizar dados da menor categoria. Estabeleceu-se como parâmetro que a menor categoria seja aumentada para um total de aproximadamente 20% do dataset. Como originalmente o dataset contém 227 observações positivas em um total de 100.000 observações (ou seja, 0,227%), é necessário aumentar o número de observações positivas em uma razão de 100, ou 10.000%.

Foi utilizado o Azure Machine Learning para balanceamento do dataset.

```{r, echo=FALSE}
# Balanceando o dataset
# Criando um arquivo CSV para carregar no Azure ML
write.csv(dados, "dados_para_balancear.csv", row.names = FALSE)
# Lendo um arquivo CSV retornado pelo Azure ML
dados <- read.csv("dados_balanceados.csv")
str(dados)
# Retornando o label para o tipo fator
dados$is_attributed <- factor(dados$is_attributed)

################################################################
######### Alternativa ao Azure ML ##############################
##### Implementar na versão 2.0: utilizar o SMOTE no pacote DMwR
################################################################

# Inspeção gráfica após o balanceamento

ggplot(dados, aes(is_attributed)) + geom_bar(fill = "blue", alpha = 0.5) +
  ggtitle("Totais de is_attributed por categoria após balanceamento")

table(dados$is_attributed)

variaveis <- names(dados)
variaveis <- variaveis[variaveis != "is_attributed"]


lapply(variaveis, function(x){
  ggplot(dados, aes_string(x)) +
    geom_histogram(fill = "blue", alpha = 0.5) +
    ggtitle(paste("Histograma de",x,"após balanceamento")) +
    facet_grid(dados$is_attributed ~ .)})

```

Visualmente nota-se padrões diferentes de distribuição nos histogramas para os atributos ip, app, channel, minute e second.

Parte-se agora para a construção e testes dos modelos de machine learning, onde serão utilizados 2 tipos de algoritmos: randomForest e SVM. Ambos os algoritmos serão testados com os atributos do tipo inteiro, como estavam originalmente, e também categorizados.


```{r}
## Criando um dataset categorizado para posterior teste dos modelos
variaveis <- names(dados)
dados_cat <- dados

for(v in variaveis){
  dados_cat[[v]] <- factor(dados_cat[[v]])
}
str(dados_cat)
```

Percebe-se que numa primeira tentativa, os atributos ip, app, device, os, channel, minute e second extrapolam o número máximo de 53 níveis do randomForest. Serão utilizados 53 níveis para estas variáveis.

```{r}

dados_cat <- dados
variaveis <- names(dados)
variaveis <- variaveis[!(variaveis %in% c("is_attributed", "hour"))]

for(v in variaveis){
  dados_cat[[v]] <- cut(dados[[v]], 53)
}

# Categorizando também o atributo hour, que tem menos que 53 níveis
dados_cat$hour <- factor(dados_cat$hour)

str(dados_cat)
str(dados)

```

### Construção dos modelos de machine learning

Antes do treinamento e avaliação dos modelos é necessário dividir os dados em dados de treino e validação, uma vez que os dados de teste fornecidos não têm label.


```{r}

# Fazendo o split data entre dados de treino e dados de validação, uma vez que os dados
# de teste não tem label disponível.

split <- createDataPartition(y = dados$is_attributed, p = 0.7, list = FALSE)
dados_treino <- dados[split, ]
dados_valid <- dados[-split, ]

dados_cat_treino <- dados_cat[split, ]
dados_cat_valid <- dados_cat[-split, ]

```

#### Primeiro modelo de machine learning
Algoritmo: randomForest
Atributos: não-categorizados

```{r}

###################################################
#### Primeiro modelo de Machine Learning ##########
###################################################
# Algoritmo: randomForest
# Atributos: não-categorizados

modelo_rf1 <- randomForest(is_attributed ~ .,
                           data = dados_treino,
                           ntree = 100, nodesize = 10, importance = T)

previsao_rf1 <- predict(modelo_rf1, dados_valid, type = 'class')


# Matriz de confusão
confusionMatrix(previsao_rf1, dados_valid$is_attributed)

```

#### Segundo modelo de machine learning
Algoritmo: randomForest
Atributos: categorizados

```{r}

###################################################
#### Segundo modelo de Machine Learning ###########
###################################################
# Algoritmo: randomForest
# Atributos: categorizados

modelo_rf2 <- randomForest(is_attributed ~ .,
                           data = dados_cat_treino,
                           ntree = 100, nodesize = 10, importance = T)

previsao_rf2 <- predict(modelo_rf2, dados_cat_valid, type = 'class')



# Matriz de confusão
confusionMatrix(previsao_rf2, dados_cat_valid$is_attributed)


```

#### Terceiro modelo de machine learning
Algoritmo: SVM
Atributos: não-categorizados

```{r}

###################################################
#### Terceiro modelo de Machine Learning ##########
###################################################
# Algoritmo: SVM
# Atributos: não-categorizados

modelo_svm1 <- svm(is_attributed ~ .,
                           data = dados_treino,
                            type = 'C-classification',
                            kernel = 'radial')

previsao_svm1 <- predict(modelo_svm1, dados_valid, type = 'class')


# Matriz de confusão
confusionMatrix(previsao_svm1, dados_valid$is_attributed)

```

#### Quarto modelo de machine learning
Algoritmo: SVM
Atributos: categorizados

```{r}
###################################################
#### Quarto modelo de Machine Learning ############
###################################################
# Algoritmo: SVM
# Atributos: categorizados

modelo_svm2 <- svm(is_attributed ~ .,
                   data = dados_cat_treino,
                   type = 'C-classification',
                   kernel = 'radial')

previsao_svm2 <- predict(modelo_svm2, dados_cat_valid, type = 'class')


# Matriz de confusão
confusionMatrix(previsao_svm2, dados_cat_valid$is_attributed)
```

#### Quinto modelo de machine learning
Algoritmo: Naive-Bayes
Atributos: não-categorizados

```{r}
###################################################
#### Quinto modelo de Machine Learning ############
###################################################
# Algoritmo: Naive-Bayes
# Atributos: não-categorizados

modelo_nb1 <- naiveBayes(dados[, -6], dados[, 6])


previsao_nb1 <- predict(modelo_nb1, dados_valid, type = 'class')


# Matriz de confusão
confusionMatrix(previsao_svm1, dados_valid$is_attributed)
```

#### Sexto modelo de machine learning
Algoritmo: Naive-Bayes
Atributos: categorizados

```{r}
###################################################
#### Sexto modelo de Machine Learning #############
###################################################
# Algoritmo: Naive-Bayes
# Atributos: categorizados

modelo_nb2 <- naiveBayes(dados_cat[, -6], dados[, 6])


previsao_nb2 <- predict(modelo_nb2, dados_cat_valid, type = 'class')


# Matriz de confusão
confusionMatrix(previsao_svm1, dados_valid$is_attributed)
```

Melhor modelo em performance: primeiro modelo, com algoritmo randomForest, treinado com os dados não-categorizados.

Fazendo as previsões no arquivo de submissão ao Kaggle.

```{r}
###################################################
#### Predição do arquivo test.csv #################
###################################################

# Melhor modelo: modelo_rf1

test <- as.data.frame(fread("test.csv"))
submission <- as.data.frame(fread("sample_submission.csv"))
head(test)
head(submission)
str(test)
str(submission)

# Para adequar o dataset de teste ao dataset de treinamento do modelo 1
# é necessário excluir a variável click_id e formatar a variável click_time
test$click_id <- NULL

test$click_time_posixct <- as.POSIXct(test$click_time)
test$click_time <- NULL


test$hour <- hour(test$click_time_posixct)
test$minute <- minute(test$click_time_posixct)
test$second <- second(test$click_time_posixct)
test$click_time_posixct <- NULL
str(test)

previsao_final <- predict(modelo_rf1, test)
table(previsao_final)

# Gravando o arquivo submission para envio para o Kaggle
submission$is_attributed <- previsao_final
fwrite(submission, "submission.csv")

```

### Conclusão

Devido a restrições de memória da máquina local, não foi possível a utilização do arquivo completo para treinamento dos modelos de machine learning.
Dentre os modelos construídos, o modelo de melhor desempenho foi o primeiro modelo, com acurácia de 99,27%, sensitividade de 99,81% (percentual de acerto para a classe 0) e especificidade de 96,93% (percentual de acerto para a classe 1). Para todos os algoritmos testados, a fatorização dos dados de entrada piorou os desempenhos dos modelos. Também em todos os outros modelos foi observada uma queda significativa na especificidade em relação ao modelo 1, fato este que não foi observado na sensitividade, provavelmente devido ao número reduzido de dados da classe 1 disponíveis.
Possíveis pontos de melhoria a serem testados poderiam ser um feature selection para otimização dos dados de entrada, bem como utilizar a penalização para erros no algoritmo random forest através do modelo C5.0.
